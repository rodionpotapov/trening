import numpy as np

from ml.grad_and_other.GD_w import gradient


# исходная функция, которую нужно аппроксимировать моделью a(x)
def func(x):
    return 0.5 * x**2 - 0.1 * 1/np.exp(-x) + 0.5 * np.cos(2*x) - 2.


# здесь объявляйте необходимые функции


coord_x = np.arange(-5.0, 5.0, 0.1) # значения по оси абсцисс [-5; 5] с шагом 0.1
coord_y = func(coord_x) # значения функции по оси ординат

sz = len(coord_x)	# количество значений функций (точек)
eta = np.array([0.01, 0.001, 0.0001, 0.01, 0.01]) # шаг обучения для каждого параметра w0, w1, w2, w3, w4
w = np.array([0., 0., 0., 0., 0.]) # начальные значения параметров модели
N = 500 # число итераций алгоритма SGD
lm = 0.02 # значение параметра лямбда для вычисления скользящего экспоненциального среднего


np.random.seed(0) # генерация одинаковых последовательностей псевдослучайных чисел

Qe = 0

for i in range(N):
    k = np.random.randint(0, sz)  # sz - размер выборки (массива coord_x)
    xk = coord_x[k] #рандомная выборка
    xi = np.array([1,xk, xk**2,np.cos(2*xk), np.sin(2*xk) ]) #массив значений х в функции a(x)
    yi = coord_y[k] #из массива целевых значений берем k образ как для xi
    err = w.T @ xi - yi # даем предсказание в xi точке (скаляр - скаляр)
    grad = 2 * err * xi
    w -= eta*grad
    Qe = lm * (w.T @ xi - yi)**2 + (1-lm) * Qe #значение Эксп. Скольз. Среднее

X = np.column_stack([np.ones_like(coord_x), coord_x, coord_x**2, np.cos(2*coord_x), np.sin(2*coord_x)]) #итоговая матрица признаков
Q = np.mean((X @ w - coord_y)**2) #итоговое значение СЭР





