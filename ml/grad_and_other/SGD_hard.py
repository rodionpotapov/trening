import numpy as np


# исходная функция, которую нужно аппроксимировать моделью a(x)
def func(x):
    return 0.5 * x + 0.2 * x ** 2 - 0.05 * x ** 3 + 0.2 * np.sin(4 * x) - 2.5


coord_x = np.arange(-4.0, 6.0, 0.1) # значения по оси абсцисс [-4; 6] с шагом 0.1
coord_y = func(coord_x) # значения функции по оси ординат

sz = len(coord_x)	# количество значений функций (точек)
eta = np.array([0.1, 0.01, 0.001, 0.0001]) # шаг обучения для каждого параметра w0, w1, w2, w3
w = np.array([0., 0., 0., 0.]) # начальные значения параметров модели
N = 500 # число итераций алгоритма SGD
lm = 0.02 # значение параметра лямбда для вычисления скользящего экспоненциального среднего
batch_size = 50 # размер мини-батча (величина K = 50)

np.random.seed(0) # генерация одинаковых последовательностей псевдослучайных чисел
Qe = 0.0

for i in range(N):
    k = np.random.randint(0, sz - batch_size)  # sz - размер выборки (массива coord_x)

    # мини-батч значений x и соответствующих y
    xk = coord_x[k:k+batch_size]                         # shape: (K,)
    yi = coord_y[k:k+batch_size]                         # shape: (K,)

    # матрица признаков батча: каждая строка = [1, x, x^2, x^3]
    xi = np.column_stack([np.ones_like(xk), xk, xk**2, xk**3])   # shape: (K, 4) матрица значений рандомно выбранных иксов в функции аппроксимации а(х)

    # ошибка на батче и градиент усеченного риска
    err = xi @ w - yi                                      # shape: (K,)
    grad = (2.0 / batch_size) * (xi.T @ err)               # shape: (4,)

    # шаг SGD
    w -= eta * grad

    # EMA батч-лосса
    Qe = lm * (err**2).mean() + (1 - lm) * Qe

# финальная матрица признаков для всей выборки и итоговый риск
X = np.column_stack([np.ones_like(coord_x), coord_x, coord_x**2, coord_x**3])
Q = np.mean((X @ w - coord_y)**2)